<!DOCTYPE html>
<html>
<head>
  <title>Logistic Regression from Scratch – Gogul Ilango </title>
  
  <link rel="shortcut icon" type="image/png" href="/images/favicon_gi.png"/>
  <link rel="stylesheet" href="https://use.typekit.net/mry7nes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@300;400;500&display=swap" rel="stylesheet">

  <script type="text/javascript" src="/js/theme.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=.5, maximum-scale=12.0, minimum-scale=.25, user-scalable=yes"/>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    <meta property="og:title" content="Logistic Regression from Scratch" />
    <meta name="description" content="Understand how to solve a classification problem using logistic regression from scratch using python and numpy." />
    <meta property="og:description" content="Understand how to solve a classification problem using logistic regression from scratch using python and numpy." />
    <meta property="twitter:title" content="Logistic Regression from Scratch" />

    <meta property="og:image" content="https://drive.google.com/uc?id=1rjTumTjtBj7nRdADfGiXs22NcX8xBwe2"/>
    <meta property="og:image:width" content="180" />
    <meta property="og:image:height" content="110" />

    <meta name="twitter:title" content="Logistic Regression from Scratch">
    <meta name="twitter:description" content="Understand how to solve a classification problem using logistic regression from scratch using python and numpy.">
    <meta name="twitter:image" content="https://drive.google.com/uc?id=1rjTumTjtBj7nRdADfGiXs22NcX8xBwe2">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Logistic Regression from Scratch | Gogul Ilango</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Logistic Regression from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understand how to solve a classification problem using logistic regression from scratch using python and numpy." />
<meta property="og:description" content="Understand how to solve a classification problem using logistic regression from scratch using python and numpy." />
<link rel="canonical" href="http://localhost:4000/software/ml/logistic-regression-from-scratch" />
<meta property="og:url" content="http://localhost:4000/software/ml/logistic-regression-from-scratch" />
<meta property="og:site_name" content="Gogul Ilango" />
<meta property="og:image" content="https://drive.google.com/uc?id=1rjTumTjtBj7nRdADfGiXs22NcX8xBwe2" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-09-28T00:00:00+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://drive.google.com/uc?id=1rjTumTjtBj7nRdADfGiXs22NcX8xBwe2" />
<meta property="twitter:title" content="Logistic Regression from Scratch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-09-28T00:00:00+05:30","datePublished":"2019-09-28T00:00:00+05:30","description":"Understand how to solve a classification problem using logistic regression from scratch using python and numpy.","headline":"Logistic Regression from Scratch","image":"https://drive.google.com/uc?id=1rjTumTjtBj7nRdADfGiXs22NcX8xBwe2","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/software/ml/logistic-regression-from-scratch"},"url":"http://localhost:4000/software/ml/logistic-regression-from-scratch"}</script>
<!-- End Jekyll SEO tag -->


  <!--[if lt IE 9]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <!--[if lte IE 8]><script type="text/javascript" src="excanvas.js"></script><![endif]-->
    <meta name="theme-color" content="#000" />
    <link id="main-style-sheet" rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Gogul Ilango - This blog is a personal space of Gogul Ilango who writes about technology, music production, programming and travel." href="/feed.xml" />

  </head>

  <body>
    <div class="outer-wrapping">
      <div class="inner-container">
          <div class="topnav header-right" id="top_navigator">
            <a title="home" id = "nav_home" href="/"><span>home</span></a>
            <a title="music" id = "nav_music" href="https://www.youtube.com/gogulilangomusic" target="_blank"><span>music</span></a>
            <a title="theme" id = "nav_theme" class = "nav_theme" onclick="switchTheme(1)"><span>theme</span></a>
            <a href = "javascript:void(0);" style="font-size:14px;" class="icon" onclick="top_navigation()">&#9776;</a>
          </div>
      </div>
    </div>

    <div id="main" role="main">
      <div class="readtime-progress" id="readtime-progress"></div>

<div class="post-heading post-heading-wrapper post-image">
	<div class="grad-post">
		<h1>Logistic Regression from Scratch</h1>
		<div class="post-subheading">
			<p>Machine Learning | 28 September 2019</p>
			<p><a href="#show_comments" id="comment-count" class="disqus-comment-count" data-disqus-url="https://gogulilango.com/software/ml/logistic-regression-from-scratch"></a></p>
		</div>
		
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

<div class="share-it-box">
<div id="share-box"> 
		<a href="whatsapp://send?text=http://localhost:4000/software/ml/logistic-regression-from-scratch" data-action="share/whatsapp/share"><img class="icon-whatsapp" src="/images/icons/whatsapp.png"/></a>

        <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/software/ml/logistic-regression-from-scratch" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><img class="icon-facebook" src="/images/icons/facebook.png"/></a>
       
        <a href="https://twitter.com/intent/tweet?text=Logistic Regression from Scratch&url=http://localhost:4000/software/ml/logistic-regression-from-scratch" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><img class="icon-twitter" src="/images/icons/twitter.png"/></a>

       <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/software/ml/logistic-regression-from-scratch&title=Logistic Regression from Scratch&summary=Understand how to solve a classification problem using logistic regression from scratch using python and numpy.&source=webjeda" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><img class="icon-linkedin" src="/images/icons/linkedin.png"/></a>                               
</div>
</div>
	</div>
</div>

<div class="containing">
	<div class="wrapping">

		<!--<div class="share-box">
	<button class="top-share-fab" id="top-share-fab" onclick="showShareBox(this.id)"></button>
	<div id="top-share-box" class="top-share-box">
		<h5>Share</h5>
		<ul>
			<li class="whatsapp-white"><a href="whatsapp://send?text=http://localhost:4000/software/ml/logistic-regression-from-scratch" data-action="share/whatsapp/share">WhatsApp</a></li>
			<li class="facebook-white"><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/software/ml/logistic-regression-from-scratch" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" >Facebook</a></li>
			<li class="twitter-white"><a href="https://twitter.com/intent/tweet?text=Logistic Regression from Scratch&url=http://localhost:4000/software/ml/logistic-regression-from-scratch" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;">Twitter</a></li>
			<li class="linkedin-white"><a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/software/ml/logistic-regression-from-scratch&title=Logistic Regression from Scratch&summary=Understand how to solve a classification problem using logistic regression from scratch using python and numpy.&source=webjeda" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" >LinkedIn</a></li>
		</ul>
	</div>
</div>-->

		<div class="post-cover">
			<div class="carbon_advertisement">
				<div class="carbon_advertisement_wrapper">
					<script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CK7I623I&placement=gogul09githubio" id="_carbonads_js"></script>
				</div>
			</div>
			<article class="post">
				<div class="entry">
					<div class="sidebar_tracker" id="sidebar_tracker">
   <button onclick="closeSidebar('sidebar_tracker_content')">X</button>
   <p onclick="showSidebar('sidebar_tracker_content')">Contents</p>
   <ul id="sidebar_tracker_content">
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_1" href="#dataset">Dataset</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_2" href="#supervised-learning">Supervised Learning</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_3" href="#linear-predictor">Linear Predictor (score)</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_4" href="#link-function">Link Function</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_5" href="#compute-likelihood">Compute Likelihood</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_6" href="#compute-derivative">Compute Derivative</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_7" href="#gradient-ascent">Gradient Ascent</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_8" href="#split-the-dataset">Split the Dataset</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_9" href="#train-the-classifier">Train the Classifier</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_10" href="#test-the-classifier">Test the Classifier</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_11" href="#reduce-overfitting">Reduce Overfitting</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_12" href="#l2-regularization">L2 Regularization</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_13" href="#conclusion">Conclusion</a></li>
    <li><a class="sidebar_links" onclick="handleSideBarLinks(this.id)" id="link_14" href="#references">References</a></li>
  </ul>
</div>

<p>During my journey as a Machine Learning (ML) practitioner, I found it has become ultimately easy for any human with limited knowledge on algorithms to take advantage of free python libraries such as <a href="https://scikit-learn.org/" target="_blank">scikit-learn</a> to solve a ML problem. Truth be said, it’s easy and sometimes no brainer to achieve this, as there are so many codes available in GitHub, Medium, Kaggle etc., You just need some amount of time looking at these codes to arrive at a solution to a problem of your choice.</p>

<p>But, what if we learn every algorithm or procedures behind each machine learning pipeline that does all the heavy lifting for us inside these amazing libraries. In this blog post and the series of blog posts to come, I will be focusing on implementing machine learning algorithms from scratch using python and numpy.</p>

<div class="note">
<p>Sure you might argue with me for the first paragraph. But learning how each algorithm work behind the scenes is very important to use these algorithms and bring in customized features in any domain (say ASIC design).</p>
</div>

<p>In this blog post, we will implement logistic regression from scratch using python and numpy to a binary classification problem.</p>

<p>I assume that you have knowledge on python programming and scikit-learn, because at the end we will compare our implementation (from scratch) with scikit-learn’s implementation.</p>

<h3 id="dataset">Dataset</h3>

<p>We will use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" target="_blank">breast cancer dataset</a> from scikit-learn for this implementation. This is a binary classification problem i.e., each data point in the training data belong to one of two classes. Below code loads the dataset and prints out the important information we seek.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="n">data</span> <span class="o">=</span> <span class="nf">load_breast_cancer</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No.of.data points (rows) : {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No.of.features (columns) : {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No.of.classes            : {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class names              : {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">)))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext code-out highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])

No.of.data points (rows) : 569
No.of.features (columns) : 30
No.of.classes            : 2
Class names              : ['malignant', 'benign']
</pre></td></tr></tbody></table></code></pre></div></div>

<p>If you wish to know more about the dataset, use <span class="coding">data.DESCR</span> to print out the entire description of the dataset.</p>

<p>As you can see, the dataset has <span class="coding">data</span> and <span class="coding">target</span> keys from which we can access the data in this dataset. There are 569 rows and 30 columns. To describe the dataset mathematically, we will use this notation \( [x_i, h(x_i), y_i] \).</p>

<p>where</p>

<ul>
  <li>\( x_i \) denotes a single data point in the dataset.</li>
  <li>\( h(x_i) \) is th feature vector for that single data point which has \( [h_1(x_i), h_2(x_i) … h_{30}(x_i)] \).</li>
  <li>\( y_i \) is the target class for that single data point.</li>
</ul>

<p>We can easily convert this dataset into a pandas dataframe for better data analysis. To view the datatype of each column, we use the below code. As every column is numeric (float64), we don’t want to perform any data preprocessing here.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext code-out highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre>0     float64
1     float64
2     float64
3     float64
4     float64
5     float64
6     float64
7     float64
8     float64
9     float64
10    float64
11    float64
12    float64
13    float64
14    float64
15    float64
16    float64
17    float64
18    float64
19    float64
20    float64
21    float64
22    float64
23    float64
24    float64
25    float64
26    float64
27    float64
28    float64
29    float64
dtype: object
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="supervised-learning">Supervised Learning</h3>

<p>In a nutshell, what we try to solve in this problem is -</p>
<ul>
  <li>We have some training data <span class="coding">X_train</span> along with its class names <span class="coding">y_train</span>.</li>
  <li>We train a model (set of algorithms) with this training data (the magic happens here and we are yet to see it!)</li>
  <li>We use the trained model to predict the class <span class="coding">y_test</span> of unseen data point <span class="coding">X_test</span>.</li>
</ul>

<p>This is called supervised learning problem (if you don’t know yet) because we use a training dataset with class names already made available to us (nicely).</p>

<p>The flowchart that you could expect before diving into logistic regression implementation might look like the one shown below.</p>

<figure>
  <img src="https://drive.google.com/uc?id=1U0Jfye7G3zKZRAoHVpHYSrhj25_hnJl-" />
  <figcaption>Figure 1. Supervised Learning using Logistic Regression.</figcaption>
</figure>

<p>Logistic regression is a type of <a href="https://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank">generalized linear classification algorithm</a> which follows a beautiful procedure to learn from data. To learn means,</p>

<ul>
  <li><strong>Weight</strong>: We define a weight value (parameter) for each feature (column) in the dataset.</li>
  <li><strong>Linear Predictor (score)</strong>: We compute weighted sum for each data point in the dataset.</li>
  <li><strong>Link Function</strong>: We use a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function" target="_blank">link function</a> to transform that weighted sum to the probability range \( [0, 1] \).</li>
  <li><strong>Log-Likelihood</strong>: We use the log-likelihood function as the <em>quality metric</em> to evaluate the prediction of the model i.e., how well the model has predicted <span class="coding">y_predict</span> when compared with ground truth <span class="coding">y_train</span>.</li>
  <li><strong>Gradient Ascent</strong>: We use gradient ascent algorithm to <em>update the weights (parameters)</em> by trying to maximize the likelihood.</li>
  <li><strong>Prediction</strong>: We take these learned weights and make predictions when new data point is given to the model.</li>
</ul>

<div class="note">
  <p><b>Note</b>: To understand how the above steps work, we need to have some knowledge on probability, statistics, calculus and linear algebra.</p>
</div>

<h3 id="linear-predictor">Linear Predictor (score)</h3>

<p>First, we define a weight value for each column (feature) in our dataset. As we have 30 features (columns) in the breast cancer dataset, we will have 30 weights [ \( \mathbf W_1, \mathbf W_2 … \mathbf W_{30}\) ]. We compute the score (weighted sum) for each data point as follows.</p>

<div class="math-cover">
$$
\begin{align}
score &amp; = \mathbf W_0 + (\mathbf W_1 * h_1(x_i)) + (\mathbf W_2 * h_2(x_i)) + ... + (\mathbf W_{30} * h_{30}(x_i)) \\
&amp; = \mathbf W^T h(x_i)
\end{align}
$$
</div>

<p>Notice we have \( \mathbf W_0 \) with no coefficient which is called the <em>bias</em> or <em>intercept</em> which must be learnt from the training data. If you need to understand what bias is, please watch <a href="https://www.youtube.com/watch?v=EuBBz3bI-aA" target="_blank">this</a> excellent video.</p>

<p>As we have numeric values, the score for each data point might fall within a range \( [-\infty, +\infty]\). Recall that our aim is to predict <em>“given a new data point, tell me whether it’s malignant (0) or benign (1)”</em>. This means, prediction from the ML model must be either 0 or 1. How are we going to achieve this? The answer is <em>link function</em>.</p>

<h3 id="link-function">Link Function</h3>

<p>If you give any input to a link function (say sigmoid), it transforms that input value to a range \( [0, 1] \). In our case, anything below 0.5 is assumed to be malignant, and anything above or equal to 0.5 is assumed to be benign.</p>

<figure>
  <img src="https://drive.google.com/uc?id=1NJSBDykPDbXtasFD6RP93M6auznJviDS" />
  <figcaption>Figure 2. Link Function over a linear predictor (score).</figcaption>
</figure>

<p>Below code is used to find the sigmoid value for a given input score.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">score</span><span class="p">):</span>
  <span class="nf">return </span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">predict_probability</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="k">return</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>In the above code, <span class="coding">features</span>, <span class="coding">weights</span> and <span class="coding">score</span> correspond to the matrices shown below.</p>

<div class="math-cover">
$$
\begin{align}
[features] = \begin{bmatrix}
    h(x_1)^T \\
    h(x_2)^T \\
    . \\
    . \\
    . \\
    h(x_{569})^T \\
    \end{bmatrix}
    = \begin{bmatrix}
    h_0(x_1) &amp; h_1(x_1) &amp; . &amp; . &amp; . &amp; h_{30}(x_1) \\
    h_0(x_2) &amp; h_1(x_2) &amp; . &amp; . &amp; . &amp; h_{30}(x_2) \\
    . &amp; . &amp; . &amp; . &amp; . &amp; . \\
    . &amp; . &amp; . &amp; . &amp; . &amp; . \\
    . &amp; . &amp; . &amp; . &amp; . &amp; . \\
    h_0(x_{569}) &amp; h_1(x_{569}) &amp; . &amp; . &amp; . &amp; h_{30}(x_{569}) \\
    \end{bmatrix}
\end{align}
$$
</div>

<div class="math-cover">
$$
[score] = [features] \mathbf w
   = \begin{bmatrix}
    h(x_1)^T \\
    h(x_2)^T \\
    . \\
    . \\
    . \\
    h(x_{569})^T 
    \end{bmatrix} \mathbf w
    = \begin{bmatrix}
    h(x_1)^T \mathbf w \\
    h(x_2)^T \mathbf w \\
    . \\
    . \\
    . \\
    h(x_{569})^T \mathbf w
    \end{bmatrix} 
    = \begin{bmatrix}
    \mathbf w^T h(x_1) \\
    \mathbf w^T h(x_2) \\
    . \\
    . \\
    . \\
    \mathbf w^T h(x_{569})
    \end{bmatrix}
$$
</div>

<p>But wait! how will the output value of this link function be the same as the ground truth value for a particular data point? It can’t be as we are randomizing the weights for the features which will throw out some random value as the prediction.</p>

<p>The whole point in learning algorithm is to <em>adjust these weights</em> based on the training data to arrive at a <em>sweet spot</em> that makes the ML model have <em>low bias</em> and <em>low variance</em>.</p>

<div class="note">
  <p>Training the classifier = Learning the weight coefficients (with low bias and low variance).</p>
</div>

<p>How do we adjust these weights? We need to define a <em>quality metric</em> that compares the output prediction of the ML model with the original ground truth class value.</p>

<p>After evaluating the quality metric, we use <em>gradient ascent algorithm</em> to update the weights in a way that the quality metric reaches a global optimum value. Interesting isn’t it?</p>

<h3 id="compute-likelihood">Compute Likelihood</h3>

<p>How do we measure “how well the classifier fits the training data”? Using <a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank">likelihood</a>. We need to choose weight coefficients \(\mathbf w\) that maximizes likelihood given below.</p>

<div class="math-cover">
$$
\prod_{i=1}^N P(y_i | \mathbf x_i, \mathbf w)
$$
</div>

<p>For a binary classification problem, it turns out that we can use <a href="https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood" target="_blank">log-likelihood</a> as the quality metric which makes computations and derivatives simpler.</p>

<div class="math-cover">
$$
l(\mathbf w) = ln \prod_{i=1}^N P(y_i | \mathbf x_i, \mathbf w)
$$
</div>

<p>After picking the log-likelihood function, we must know it’s derivative with respect to a weight coefficient so that we can use gradient ascent to update that weight.</p>

<p>We use the below equation to calculate the log-likelihood for the classifier.</p>

<div class="math-cover">
$$
ll(\mathbf w) = \sum_{i=1}^N ((\mathbf 1[y_i = +1] - 1) \mathbf w^T h(\mathbf w_i) - ln(1 + exp(-\mathbf w^T h(x_i))))
$$
</div>

<p>We will understand the formation of these equations in a separate post. But for now, let us focus on implementing everything in code.</p>

<p>We define the below function to compute log-likelihood. Notice that we sum over all the training examples.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">compute_log_likelihood</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="n">indicator</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span><span class="o">==+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">scores</span>    <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">ll</span>        <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">indicator</span><span class="p">]))</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">scores</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">ll</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="compute-derivative">Compute Derivative</h3>

<p>Once we have the log-likelihood equation, we can compute its derivative with respect to a single weight coefficient using the below formula.</p>

<div class="math-cover">
$$
\frac{\partial l}{\partial w_j} = \sum_{i=1}^N h_j(\mathbf x_i) (\mathbf 1[y_i = +1] - P(y_i = +1|\mathbf x_i, \mathbf w))
$$
</div>

<p>The above equation might look scary. But its easy to write in code.</p>

<ul>
  <li>The term \((\mathbf 1[y_i = +1] - P(y_i = +1|\mathbf x_i, \mathbf w)\) is nothing but the difference between <span class="coding">indicators</span> and <span class="coding">predictions</span> which is equal to <span class="coding">errors</span>.</li>
  <li>\(h_j(\mathbf x_i)\) is the feature value of a training example \(\mathbf x_i \) for a single column \(j\).</li>
</ul>

<p>We find the derivative of log-likelihood with respect to each of the weight coefficient \( \mathbf w \) which in turn depends on its feature column.</p>

<p>Notice that we sum over all the training examples, and the derivative that we return is a single number.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">feature_derivative</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">feature</span><span class="p">):</span>
  <span class="n">derivative</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">errors</span><span class="p">),</span> <span class="n">feature</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">derivative</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="gradient-ascent">Gradient Ascent</h3>

<p>Now, we have all the ingredients to perform gradient ascent. The magic of this tutorial happens here!</p>

<p>Think of gradient ascent similar to hill-climbing. To reach the top of the hill (which is the global maximum), we choose a parameter called <em>learning-rate</em>. This defines the <em>step-size</em> that we need to take each iteration before we update the weight coefficients.</p>

<p>The steps that we will perform in gradient ascent are as follows.</p>

<ol>
  <li>Initialize weights vector \( \mathbf w \) to random values or zero using <span class="coding">np.zeros()</span>.</li>
  <li>Predict the class probability \( P(y_i = +1|\mathbf x_i, \mathbf w) \) for all training examples using <span class="coding">predict_probability</span> function and save to a variable named <span class="coding">predictions</span>. The shape of this variable would be <span class="coding">y_train.shape</span>.</li>
  <li>Calculate the indicator value for all training examples by comparing the label against \( +1 \) and save it to a variable named <span class="coding">indicators</span>. The shape of this variable would also be <span class="coding">y_train.shape</span>.</li>
  <li>Calculate the errors as the difference between <span class="coding">indicators</span> and <span class="coding">predictions</span> and save it to a variable named <span class="coding">errors</span>.</li>
  <li><strong>Important step</strong>: For each \( j^{th} \) weight coefficient, compute it’s derivative using <span class="coding">feature_derivative</span> function with the \( j^{th} \) column of features. Increment the \( j^{th} \) coefficient using \( lr * derivative\) where \( lr \) is the learning rate for this algorithm which we handpick.</li>
  <li>Do steps 2 to 5 for <span class="coding">epochs</span> times (number of iterations) and return the learned weight coefficients.</li>
</ol>

<p>Below is the code to perform logistic regression using gradient ascent optimization algorithm.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre><span class="c1"># logistic regression without L2 regularization
</span><span class="k">def</span> <span class="nf">logistic_regression</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>

  <span class="c1"># add bias (intercept) with features matrix
</span>  <span class="n">bias</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">features</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span>

  <span class="c1"># initialize the weight coefficients
</span>  <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  <span class="n">logs</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># loop over epochs times
</span>  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># predict probability for each row in the dataset
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">predict_probability</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1"># calculate the indicator value
</span>    <span class="n">indicators</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="o">==+</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># calculate the errors
</span>    <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">indicators</span><span class="p">]))</span> <span class="o">-</span> <span class="n">predictions</span>

    <span class="c1"># loop over each weight coefficient
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>

      <span class="c1"># calculate the derivative of jth weight cofficient
</span>      <span class="n">derivative</span> <span class="o">=</span> <span class="nf">feature_derivative</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">features</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
      <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">derivative</span>

    <span class="c1"># compute the log-likelihood
</span>    <span class="n">ll</span> <span class="o">=</span> <span class="nf">compute_log_likelihood</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">logs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>

  <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">logs</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">logs</span><span class="p">))</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logs</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">'</span><span class="s">Training the classifier (without L2)</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Log-likelihood</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">train_without_l2.jpg</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">weights</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="split-the-dataset">Split the dataset</h3>

<p>To test our classifier’s performance, we will split the original dataset into training and testing. We choose a <span class="coding">test_size</span> parameter value to split the dataset into <span class="coding">train</span> and <span class="coding">test</span> using scikit-learn’s <span class="coding">train_test_split</span> function as shown below.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># split the dataset into training and testing 
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">X_train : </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">y_train : </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">X_test : </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">y_test : </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext code-out highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>X_train : (455, 30)
y_train : (455,)
X_test : (114, 30)
y_test : (114,)
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="train-the-classifier">Train the classifier</h3>

<p>As we already learnt, training the classifier means learning the weight coefficients. To train the classifier, we</p>
<ul>
  <li>Add intercept or bias to the feature matrix.</li>
  <li>Initialize the weight coefficients to zeros.</li>
  <li>Handpick the hyper-parameters <em>learning rate</em> and <em>epochs</em>.</li>
  <li>Use <span class="coding">logistic_regression()</span> function that we have just built and pass in the ingredients.</li>
</ul>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># hyper-parameters
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">epochs</span>        <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># perform logistic regression
</span><span class="n">learned_weights</span> <span class="o">=</span> <span class="nf">logistic_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<figure>
  <img src="https://drive.google.com/uc?id=1e9REgzv_FIBdyW235udH9y-BUVfkxqvC" />
  <figcaption>Figure 3. Increasing log-likelihood during training (without L2 regularization).</figcaption>
</figure>

<h3 id="test-the-classifier">Test the classifier</h3>

<p>To make predictions using the trained classifier, we use <span class="coding">X_test</span> data (testing data), <span class="coding">learned_weights</span> and <span class="coding">predict_probability()</span> function.</p>

<p>To find the accuracy between ground truth class values <span class="coding">y_test</span> and logistic regression predicted class values <span class="coding">predictions</span>, we use scikit-learn’s <span class="coding">accuracy_score()</span> function as shown below.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="c1"># make predictions using learned weights on testing data
</span><span class="n">bias_train</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">bias_test</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">features_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">))</span>
<span class="n">features_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">))</span>

<span class="n">test_predictions</span>  <span class="o">=</span> <span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">learned_weights</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span> <span class="n">learned_weights</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of our LR classifier on training data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_predictions</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of our LR classifier on testing data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_predictions</span><span class="p">)))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext code-out highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Accuracy of our LR classifier on training data: 0.9164835164835164
Accuracy of our LR classifier on testing data: 0.9298245614035088
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="reduce-overfitting">Reduce Overfitting</h3>

<p>Overfitting is a mandatory problem that we need to solve when it comes to machine learning. After training, we have the learned weight coefficients which must not <em>overfit</em> the training dataset.</p>

<p>When the decision boundary traced by the learned weight coefficients fits the training data extremely well, we have this overfitting problem. Often, overfitting is associated with very large estimated weight coefficients. This leads to overconfident predictions which is not very good for a real-world classifier.</p>

<p>To solve this, we need to measure the magnitude of weight coefficients. There are two approaches to measure it.</p>

<p><strong>L1 norm</strong>: Sum of absolute value</p>

<p>\(\lVert \mathbf w \rVert _1 = |\mathbf w_0| + |\mathbf w_1| + |\mathbf w_2| … + |\mathbf w_N| \)</p>

<p><strong>L2 norm</strong>: Sum of squares</p>

<p>\(\lVert \mathbf w \rVert _2^2 = \mathbf w_0^2 + \mathbf w_1^2 + \mathbf w_2^2 … + \mathbf w_N^2 \)</p>

<h3 id="l2-regularization">L2 Regularization</h3>

<p>We will use L2 norm (sum of squares) to reduce overshooting weight coefficients. It turns out that, instead of using likelihood function alone as the quality metric, what if we subtract \(\lambda \lVert \mathbf w \rVert _2^2\) from it, where \(\lambda\) is a hyper-parameter to control bias-variance tradeoff due to this regularization.</p>

<p>So, our new quality metric with regularization to combat overconfidence problem would be</p>

<div class="math-cover">
$$
l(w) - \lambda \lVert \mathbf w \rVert _2^2
$$
</div>

<ul>
  <li>Large \(\lambda \): High bias, low variance.</li>
  <li>Small \(\lambda \): Low bias, high variance.</li>
</ul>

<p>Recall to perform gradient ascent, we need to know the derivative of quality metric to update the weight coefficients. Thus, the new derivative equation would be</p>

<div class="math-cover">
$$
\frac{\partial l(\mathbf w)}{\partial \mathbf w_j} - 2 \lambda \mathbf w_j
$$
</div>

<p>Let’s understand the regularization impact on penalizing weight coefficients.</p>

<ul>
  <li>If \( \mathbf w_j &gt; 0\), then \(- 2 \lambda \mathbf w_j &lt; 0\), thus it decreases \( \mathbf w_j &gt; 0\) resulting in \( \mathbf w_j \) closer to 0.</li>
  <li>If \( \mathbf w_j &lt; 0\), then \(- 2 \lambda \mathbf w_j &gt; 0\), thus it increases \( \mathbf w_j &gt; 0\) resulting in \( \mathbf w_j \) closer to 0.</li>
</ul>

<p>When it comes to code, we need to update <span class="coding">feature_derivative()</span> function, <span class="coding">compute_log_likelihood()</span> function and <span class="coding">logistic_regression()</span> function with whatever we have learnt so far about L2 regularization as shown below.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
</pre></td><td class="rouge-code"><pre><span class="c1"># feature derivative computation with L2 regularization
</span><span class="k">def</span> <span class="nf">l2_feature_derivative</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">l2_penalty</span><span class="p">,</span> <span class="n">feature_is_constant</span><span class="p">):</span>
  <span class="n">derivative</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">errors</span><span class="p">),</span> <span class="n">feature</span><span class="p">)</span>
  
  <span class="k">if</span> <span class="ow">not</span> <span class="n">feature_is_constant</span><span class="p">:</span>
    <span class="n">derivative</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">l2_penalty</span> <span class="o">*</span> <span class="n">weight</span>

  <span class="k">return</span> <span class="n">derivative</span>

<span class="c1"># log-likelihood computation with L2 regularization
</span><span class="k">def</span> <span class="nf">l2_compute_log_likelihood</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">l2_penalty</span><span class="p">):</span>
  <span class="n">indicator</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span><span class="o">==+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">scores</span>    <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">ll</span>        <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">indicator</span><span class="p">]))</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">scores</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="p">)))</span> <span class="o">-</span> <span class="p">(</span><span class="n">l2_penalty</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">ll</span>

<span class="c1"># logistic regression with L2 regularization
</span><span class="k">def</span> <span class="nf">l2_logistic_regression</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">l2_penalty</span><span class="p">):</span>

  <span class="c1"># add bias (intercept) with features matrix
</span>  <span class="n">bias</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">features</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span>

  <span class="c1"># initialize the weight coefficients
</span>  <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  <span class="n">logs</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># loop over epochs times
</span>  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># predict probability for each row in the dataset
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">predict_probability</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="c1"># calculate the indicator value
</span>    <span class="n">indicators</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="o">==+</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># calculate the errors
</span>    <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">indicators</span><span class="p">]))</span> <span class="o">-</span> <span class="n">predictions</span>

    <span class="c1"># loop over each weight coefficient
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>

      <span class="n">isIntercept</span> <span class="o">=</span> <span class="p">(</span><span class="n">j</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>

      <span class="c1"># calculate the derivative of jth weight cofficient
</span>      <span class="n">derivative</span> <span class="o">=</span> <span class="nf">l2_feature_derivative</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">features</span><span class="p">[:,</span><span class="n">j</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">l2_penalty</span><span class="p">,</span> <span class="n">isIntercept</span><span class="p">)</span>
      <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">derivative</span>

    <span class="c1"># compute the log-likelihood
</span>    <span class="n">ll</span> <span class="o">=</span> <span class="nf">l2_compute_log_likelihood</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">l2_penalty</span><span class="p">)</span>
    <span class="n">logs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>

  <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">logs</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">logs</span><span class="p">))</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logs</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">'</span><span class="s">Training the classifier (with L2)</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Log-likelihood</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">train_with_l2.jpg</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">weights</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now, we can perform logistic regression with L2 regularization on this dataset using the below code.</p>

<div class="code-head">logistic_regression.py<span>code</span></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td><td class="rouge-code"><pre><span class="c1"># logistic regression with regularization
</span><span class="k">def</span> <span class="nf">lr_with_regularization</span><span class="p">():</span>
  <span class="c1"># hyper-parameters
</span>  <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-7</span>
  <span class="n">epochs</span>        <span class="o">=</span> <span class="mi">300000</span>
  <span class="n">l2_penalty</span>    <span class="o">=</span> <span class="mf">0.001</span>

  <span class="c1"># perform logistic regression and get the learned weights
</span>  <span class="n">learned_weights</span> <span class="o">=</span> <span class="nf">l2_logistic_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">l2_penalty</span><span class="p">)</span>

  <span class="c1"># make predictions using learned weights on testing data
</span>  <span class="n">bias_train</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">bias_test</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">features_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">))</span>
  <span class="n">features_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">bias_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">))</span>

  <span class="n">test_predictions</span>  <span class="o">=</span> <span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">learned_weights</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">train_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="nf">predict_probability</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span> <span class="n">learned_weights</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of our LR classifier on training data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_predictions</span><span class="p">)))</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of our LR classifier on testing data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_predictions</span><span class="p">)))</span>

  <span class="c1"># using scikit-learn's logistic regression classifier
</span>  <span class="n">model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">sk_test_predictions</span>  <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">sk_train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of scikit-learn</span><span class="sh">'</span><span class="s">s LR classifier on training data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">sk_train_predictions</span><span class="p">)))</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of scikit-learn</span><span class="sh">'</span><span class="s">s LR classifier on testing data: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">sk_test_predictions</span><span class="p">)))</span>

  <span class="nf">visualize_weights</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">learned_weights</span><span class="p">),</span> <span class="sh">'</span><span class="s">weights_with_l2.jpg</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># visualize weight coefficients
</span><span class="k">def</span> <span class="nf">visualize_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
  <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Weight Index (Feature Column Number)</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Weight Coefficient</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Visualizing Weights</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">lr_without_regularization</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext code-out highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>Accuracy of our LR classifier on training data: 0.9406593406593406
Accuracy of our LR classifier on testing data: 0.9385964912280702
Accuracy of scikit-learn's LR classifier on training data: 0.9648351648351648
Accuracy of scikit-learn's LR classifier on testing data: 0.9385964912280702
</pre></td></tr></tbody></table></code></pre></div></div>

<figure>
  <img src="https://drive.google.com/uc?id=1NZq0sfCxxoiw7BIkldIZ8-EdxkixUtBm" />
  <figcaption>Figure 4. Visualizing learnt weight coefficients after training.</figcaption>
</figure>

<figure>
  <img src="https://drive.google.com/uc?id=1HLvXeICcqJ2eaf21f4mLCkShGj1jOEWs" />
  <figcaption>Figure 5. Increasing log-likelihood during training (with L2 regularization).</figcaption>
</figure>

<h3 id="conclusion">Conclusion</h3>

<p>Thus, we have implemented our very own logistic regression classifier using python and numpy with/without L2 regularization, and compared it with scikit-learn’s implementation.</p>

<p>We have achieved the <strong>same test accuracy as scikit-learn’s implementation</strong> and what a way to achieve it on our own!</p>

<p>One key take away from this post is that, we still need to manually tune these hyper-parameters (<span class="coding">learning_rate</span>, <span class="coding">epochs</span> and <span class="coding">l2_penalty</span>) to reach the global maximum. If you found some approach to automate this task, please leave it out in the comments so that I as well as others can learn it.</p>

<div class="references">
<h3 id="references">References</h3>

<ul>
  <li><a href="https://www.coursera.org/learn/ml-classification" target="_blank">Coursera's Machine Learning: Classification by Professor Carlos Guestrin</a></li>
  <li><a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S05/slides/LogRegress-1-24-05.pdf" target="_blank">Logistic Regression, Generative and Discriminative Classifiers</a></li>
</ul>

</div>

				</div>
				<div class="note closers">
	<p>In case if you found something useful to add to this article or you found a bug in the code or would like to improve some points mentioned, feel free to write it down in the comments. Hope you found something useful here.</p>
</div>
			</article>
			
			<div class="show-comments" onclick="showComments()"><p id="show_comments"><span id="comment_count" class="disqus-comment-count" data-disqus-url="https://gogulilango.com/software/ml/logistic-regression-from-scratch"></span></p></div>

			<div id="disqus_thread"></div>
			<script>
				var disqus_config = function () {
				  this.page.url = 'http://localhost:4000/software/ml/logistic-regression-from-scratch';
				  this.page.identifier = 'http://localhost:4000/software/ml/logistic-regression-from-scratch';
				};

				(function() {
				  var d = document, s = d.createElement('script');
				  s.src = 'https://gogul09.disqus.com/embed.js';
				  s.setAttribute('data-timestamp', +new Date());
				  (d.head || d.body).appendChild(s);
				})();
			</script>
			<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
		</div>
	</div>
</div>

<script type="text/javascript">
	
	window.onscroll = function() {
		sideBarScrollHandler();
		windowScrollHandler();
	};
	
	function sideBarScrollHandler() {
		if (document.body.scrollTop > 350 || document.documentElement.scrollTop > 350) {
			document.getElementById("sidebar_tracker").style.top = "20px";
		} else {
			document.getElementById("sidebar_tracker").style.top = "70px";
		}
	}
	
</script>

<script type="text/javascript" src="/js/readtime.js"></script>
    </div>

    <div class="wrapper-footer">
      <footer class="footer">
        <p><span>&copy; 2024 - gogul ilango | opinions are my own</span></p>
       </footer>
     </div>

     <button onclick="topScroller()" id="btnScrollTop" title="Go to top" class="w3-animate-bottom"></button>

     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
     <script src="https://apis.google.com/js/platform.js"></script>
     <script async defer src="https://buttons.github.io/buttons.js"></script>
     <script id="dsq-count-scr" src="//gogul09.disqus.com/count.js" async></script>
     <script src="/js/custom.js"></script>
     
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-93019594-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/software/ml/logistic-regression-from-scratch',
		  'title': 'Logistic Regression from Scratch'
		});
	</script>
	<!-- End Google Analytics -->

   </body>
</html>
